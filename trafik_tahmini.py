# -*- coding: utf-8 -*-
"""trafik_tahmini.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/siracgezgin/trafik-tahmini/blob/main/trafik_tahmini.ipynb
"""

# Gerekli kütüphaneler
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler
from sklearn.model_selection import train_test_split, cross_val_score, learning_curve, GridSearchCV
from sklearn.metrics import (
    mean_absolute_error, mean_squared_error, r2_score,
    explained_variance_score, max_error, mean_absolute_percentage_error
)
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.svm import SVR
import warnings
import time

# Uyarıları kapatma
warnings.filterwarnings('ignore')

# Matplotlib tarzı ayar (geçerli bir stil seç)
plt.style.use('ggplot')

# Veri setini yükleme
file_path = "Trafik_Verileri.csv"  # Dosya yolunu güncelle
df = pd.read_csv(file_path)

# Veri setinin ilk incelenmesi
print("Veri Seti Boyutu:", df.shape)
print("\nVeri Seti Sütunları ve Türleri:")
print(df.info())
print("\nİlk 5 Satır:")
print(df.head())

# Eksik değerlerin analizi
print("\nEksik Değer Sayıları:")
print(df.isnull().sum())

# Eksik değer haritası
plt.figure(figsize=(12, 6))
sns.heatmap(df.isnull(), yticklabels=False, cbar=True, cmap='viridis')
plt.title("Eksik Değer Haritası")
plt.show()

# Veri seti boyutunun kontrolü
print("Veri Seti Boyutu:", df.shape)

# Veri türleri ve sütun bilgisi
print("\nVeri Seti Bilgisi:")
print(df.info())

# Temel istatistiksel bilgiler
print("\nVeri Seti Temel İstatistiksel Özellikleri:")
print(df.describe())

# Eksik değerlerin sayısı
print("\nEksik Değer Sayıları:")
print(df.isnull().sum())

# Kategorik sütunların kontrolü
# İlk olarak tüm sütun adlarını listeleyelim
print("\nVeri Setindeki Sütunlar:")
print(df.columns)

# Kategorik sütunlar belirleniyor
categorical_columns = ['WEATHER', 'IS_HOLIDAY', 'IS_PEAK_HOUR', 'IS_WEEKEND', 'SEASON']

# Kategorik değişkenlerde unique değerlerin kontrolü
print("\nKategorik Değişkenlerin Unique Değerleri:")
for col in categorical_columns:
    if col in df.columns:  # Sütun isimleri uyuşuyorsa kontrol et
        print(f"\n{col} sütununun unique değerleri:")
        print(df[col].value_counts())
    else:
        print(f"\n{col} sütunu veri setinde bulunamadı.")

# Sütun isimlerini kontrol edelim
print("Orijinal Sütun İsimleri:")
print(df.columns)

# Sütun isimlerini temizleyelim
df.columns = df.columns.str.strip()  # Boşlukları temizle
df.columns = df.columns.str.replace(' ', '_').str.upper()  # Boşlukları alt tire ile değiştir ve büyük harfe çevir

# Temizlendikten sonra sütun isimleri
print("\nTemizlenmiş Sütun İsimleri:")
print(df.columns)

# Tekrar kontrol ederek eksik değer durumuna bakalım
print("\nEksik Değerler Temizlendikten Sonra:")
print(df.isnull().sum())

# Eksik değer analizi ve doldurma
numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns
categorical_columns = df.select_dtypes(include=['object']).columns

# Sayısal sütunları ortalama ile doldurma
df[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].mean())

# Kategorik sütunları mod (en sık değer) ile doldurma
for col in categorical_columns:
    df[col] = df[col].fillna(df[col].mode()[0])

# Eksik değerlerin tekrar kontrol edilmesi
print("\nEksik Değerler Doldurulduktan Sonra:")
print(df.isnull().sum())

# Veri keşfi ve görselleştirme
plt.figure(figsize=(12, 6))
sns.boxplot(data=df[numeric_columns])
plt.title("Sayısal Değişkenlerin Kutu Grafiği")
plt.xticks(rotation=90)
plt.show()

plt.figure(figsize=(12, 6))
sns.histplot(df['AVERAGE_SPEED'], bins=30, kde=True)
plt.title("Ortalama Hız Dağılımı")
plt.show()

# Sayısal sütunları ortalama ile doldurma
numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns
df[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].mean())

# Kategorik sütunları mod (en sık değer) ile doldurma
categorical_columns = ['WEATHER', 'SEASON']
for col in categorical_columns:
    if col in df.columns:
        df[col] = df[col].fillna(df[col].mode()[0])

# Eksik değerlerin tekrar kontrol edilmesi
print("\nEksik Değerler Doldurulduktan Sonra:")
print(df.isnull().sum())

# DATE_TIME sütununu datetime formatına çevirme
df['DATE_TIME'] = pd.to_datetime(df['DATE_TIME'], errors='coerce')

# Yeni zaman bazlı özellikler oluşturma
df['HOUR'] = df['DATE_TIME'].dt.hour
df['DAY'] = df['DATE_TIME'].dt.day
df['MONTH'] = df['DATE_TIME'].dt.month
df['YEAR'] = df['DATE_TIME'].dt.year
df['WEEKDAY'] = df['DATE_TIME'].dt.weekday
df['IS_WEEKEND'] = df['WEEKDAY'].apply(lambda x: 1 if x >= 5 else 0)

# Zaman bazlı özelliklerin ilk birkaç satırını görüntüleme
print("\nZaman Bazlı Özellikler:")
print(df[['DATE_TIME', 'HOUR', 'DAY', 'MONTH', 'YEAR', 'WEEKDAY', 'IS_WEEKEND']].head())

# Aykırı değerlerin tespiti ve düzeltilmesi (IQR yöntemi)
def handle_outliers(data, column):
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Aykırı değerleri sınırlarla değiştirme
    data[column] = np.where(data[column] < lower_bound, lower_bound, data[column])
    data[column] = np.where(data[column] > upper_bound, upper_bound, data[column])
    return data

# Sayısal sütunlarda aykırı değerleri kontrol edip düzeltme
numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns
for col in numeric_columns:
    plt.figure(figsize=(10, 4))
    plt.subplot(1, 2, 1)
    sns.boxplot(df[col])
    plt.title(f'{col} - Temizleme Öncesi')

    df = handle_outliers(df, col)

    plt.subplot(1, 2, 2)
    sns.boxplot(df[col])
    plt.title(f'{col} - Temizleme Sonrası')
    plt.tight_layout()
    plt.show()

print("Aykırı değerler başarıyla düzeltildi.")

# StandardScaler uygulama
scaler = StandardScaler()
scaled_columns = ['LATITUDE', 'LONGITUDE', 'MINIMUM_SPEED', 'MAXIMUM_SPEED', 'AVERAGE_SPEED', 'NUMBER_OF_VEHICLES']
df[scaled_columns] = scaler.fit_transform(df[scaled_columns])

print("\nVeri Normalizasyonu Tamamlandı. İlk birkaç satıra bakalım:")
print(df[scaled_columns].head())

# Özellikler (X) ve hedef değişken (y)
X = df.drop(columns=['AVERAGE_SPEED', 'DATE_TIME', 'GEOHASH'])  # Hedef değişken hariç
y = df['AVERAGE_SPEED']  # Hedef değişken

# Eğitim ve test setlerine ayırma
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("\nEğitim ve Test Setleri:")
print(f"X_train boyutu: {X_train.shape}")
print(f"X_test boyutu: {X_test.shape}")
print(f"y_train boyutu: {y_train.shape}")
print(f"y_test boyutu: {y_test.shape}")

# Kategorik değişkenleri belirleme
categorical_columns = ['WEATHER', 'SEASON']

# One-Hot Encoding uygulama (daha iyi model performansı için)
df = pd.get_dummies(df, columns=categorical_columns, drop_first=True)

# Tekrar X ve y'yi oluşturma
X = df.drop(columns=['AVERAGE_SPEED', 'DATE_TIME', 'GEOHASH'])
y = df['AVERAGE_SPEED']

# Eğitim ve test setlerine yeniden ayırma
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("\nKategorik veriler encode edildi ve veri yeniden hazırlandı.")
print(f"Yeni X_train boyutu: {X_train.shape}")
print(f"Yeni X_test boyutu: {X_test.shape}")

from sklearn.model_selection import RandomizedSearchCV

# Model parametreleri
rf_params = {
    'n_estimators': [100, 200, 300, 400],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

gb_params = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'max_depth': [3, 5, 10]
}

svr_params = {
    'kernel': ['linear', 'poly', 'rbf'],
    'C': [0.1, 1, 10, 100],
    'epsilon': [0.1, 0.2, 0.5, 0.3]
}

# Modelleri tanımlama
models = {
    'Linear Regression': LinearRegression(),
    'Random Forest': RandomizedSearchCV(RandomForestRegressor(random_state=42), rf_params, n_iter=100, cv=5, verbose=2, random_state=42, n_jobs=-1),
    'Gradient Boosting': RandomizedSearchCV(GradientBoostingRegressor(random_state=42), gb_params, n_iter=100, cv=5, verbose=2, random_state=42, n_jobs=-1),
    'SVR': RandomizedSearchCV(SVR(), svr_params, n_iter=100, cv=5, verbose=2, random_state=42, n_jobs=-1),
    'Ridge': Ridge(random_state=42),
    'Lasso': Lasso(random_state=42)
}

# Model sonuçlarını saklamak için dictionary
results = {}

from sklearn.model_selection import RandomizedSearchCV

# Model parametreleri
rf_params = {
    'n_estimators': [50, 100],
    'max_depth': [10, 20],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
    'bootstrap': [True, False]
}

gb_params = {
    'n_estimators': [50, 100],
    'learning_rate': [0.01, 0.1],
    'max_depth': [3, 5]
}

svr_params = {
    'kernel': ['linear', 'rbf'],
    'C': [0.1, 1],
    'epsilon': [0.1, 0.2]
}

# Küçük bir veri örneği oluşturma
sample_size = 0.1
df_sample = df.sample(frac=sample_size, random_state=42)
X_sample = df_sample.drop(columns=['AVERAGE_SPEED', 'DATE_TIME', 'GEOHASH'])
y_sample = df_sample['AVERAGE_SPEED']

# Eğitim ve test setlerine ayırma
X_train_sample, X_test_sample, y_train_sample, y_test_sample = train_test_split(X_sample, y_sample, test_size=0.2, random_state=42)

# Modelleri tanımlama
models = {
    'Linear Regression': LinearRegression(),
    'Random Forest': RandomizedSearchCV(RandomForestRegressor(random_state=42), rf_params, n_iter=10, cv=3, verbose=2, random_state=42, n_jobs=-1),
    'Gradient Boosting': RandomizedSearchCV(GradientBoostingRegressor(random_state=42), gb_params, n_iter=10, cv=3, verbose=2, random_state=42, n_jobs=-1),
    'SVR': RandomizedSearchCV(SVR(), svr_params, n_iter=10, cv=3, verbose=2, random_state=42, n_jobs=-1),
    'Ridge': Ridge(random_state=42),
    'Lasso': Lasso(random_state=42)
}

# Model sonuçlarını saklamak için dictionary
results = {}

# Her model için eğitim ve değerlendirme
for name, model in models.items():
    print(f"\nModel: {name}")

    # Model eğitimi
    model.fit(X_train_sample, y_train_sample)

    # RandomizedSearchCV kullanılan modeller için en iyi parametreleri yazdırma
    if isinstance(model, RandomizedSearchCV):
        print("En iyi parametreler:", model.best_params_)
        print("En iyi cross-validation skoru:", model.best_score_)

    # Tahminler
    y_pred = model.predict(X_test_sample)

    # Metrikler
    metrics = {
        'R2 Score': r2_score(y_test_sample, y_pred),
        'MAE': mean_absolute_error(y_test_sample, y_pred),
        'RMSE': np.sqrt(mean_squared_error(y_test_sample, y_pred)),
        'Explained Variance': explained_variance_score(y_test_sample, y_pred),
        'MAPE': mean_absolute_percentage_error(y_test_sample, y_pred),
        'Cross Val Score': np.mean(cross_val_score(model, X_train_sample, y_train_sample, cv=3))
    }

    # Sonuçları yazdırma
    for metric_name, value in metrics.items():
        print(f"{metric_name}: {value:.4f}")

    results[name] = metrics

    # Tahmin vs Gerçek değer grafiği
    plt.figure(figsize=(10, 6))
    plt.scatter(y_test_sample, y_pred, alpha=0.5)
    plt.plot([y_test_sample.min(), y_test_sample.max()], [y_test_sample.min(), y_test_sample.max()], 'r--', lw=2)
    plt.xlabel('Gerçek Değerler')
    plt.ylabel('Tahmin Değerler')
    plt.title(f'{name} - Tahmin vs Gerçek')
    plt.tight_layout()
    plt.show()

# Model karşılaştırma
print("\nModel Karşılaştırma")
print("-" * 50)

# Sonuçları DataFrame'e çevirme
results_df = pd.DataFrame(results).T
print("\nTüm Model Metrikleri:")
print(results_df)

# Metrikleri görselleştirme
metrics_to_plot = ['R2 Score', 'MAE', 'RMSE', 'Cross Val Score']
fig, axes = plt.subplots(2, 2, figsize=(15, 10))
fig.suptitle('Model Karşılaştırma Metrikleri')

for i, metric in enumerate(metrics_to_plot):
    ax = axes[i//2, i%2]
    results_df[metric].plot(kind='bar', ax=ax)
    ax.set_title(metric)
    ax.set_xticklabels(ax.get_xticklabels(), rotation=45)

plt.tight_layout()
plt.show()

from sklearn.inspection import permutation_importance

# Özellik Önem Analizi
print("\nÖzellik Önem Analizi")
print("-" * 50)

# Random Forest modeli için özellik önemleri
features = X.columns  # Özellik isimlerinin alınması
rf_model = models['Random Forest'].best_estimator_ if isinstance(models['Random Forest'], RandomizedSearchCV) else models['Random Forest']
feature_importance = pd.DataFrame({
    'feature': features,
    'importance': rf_model.feature_importances_
}).sort_values('importance', ascending=False)

plt.figure(figsize=(12, 8))
sns.barplot(x='importance', y='feature', data=feature_importance)
plt.title('Özellik Önem Dereceleri (Random Forest)')
plt.show()

# SVR modeli için özellik önem analizi
print("\nSVR Modeli İçin Özellik Önem Analizi")
print("-" * 50)

svr_model = models['SVR'].best_estimator_ if isinstance(models['SVR'], RandomizedSearchCV) else models['SVR']
svr_model.fit(X_train, y_train)

# Permutation feature importance
result = permutation_importance(svr_model, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)

svr_importance = pd.DataFrame({
    'feature': features,
    'importance': result.importances_mean
}).sort_values('importance', ascending=False)

print("\nEn önemli 10 özellik (SVR):")
print(svr_importance.head(10))

plt.figure(figsize=(12, 8))
sns.barplot(x='importance', y='feature', data=svr_importance)
plt.title('Özellik Önem Dereceleri (SVR)')
plt.show()

# Gradient Boosting modeli için özellik önem analizi
print("\nGradient Boosting Modeli İçin Özellik Önem Analizi")
print("-" * 50)

gb_model = models['Gradient Boosting'].best_estimator_ if isinstance(models['Gradient Boosting'], RandomizedSearchCV) else models['Gradient Boosting']
gb_model.fit(X_train, y_train)

# Permutation feature importance
result = permutation_importance(gb_model, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)

gb_importance = pd.DataFrame({
    'feature': features,
    'importance': result.importances_mean
}).sort_values('importance', ascending=False)

print("\nEn önemli 10 özellik (Gradient Boosting):")
print(gb_importance.head(10))

plt.figure(figsize=(12, 8))
sns.barplot(x='importance', y='feature', data=gb_importance)
plt.title('Özellik Önem Dereceleri (Gradient Boosting)')
plt.show()

# Learning curves analizi
print("\nLearning Curves Analizi")
print("-" * 50)

def plot_learning_curves(model, X, y, model_name):
    train_sizes, train_scores, test_scores = learning_curve(
        model, X, y, cv=5, n_jobs=-1,
        train_sizes=np.linspace(0.1, 1.0, 10),
        scoring='r2'
    )

    train_mean = np.mean(train_scores, axis=1)
    train_std = np.std(train_scores, axis=1)
    test_mean = np.mean(test_scores, axis=1)
    test_std = np.std(test_scores, axis=1)

    plt.figure(figsize=(10, 6))
    plt.plot(train_sizes, train_mean, label='Training score')
    plt.plot(train_sizes, test_mean, label='Cross-validation score')

    plt.fill_between(train_mean - train_std, train_mean + train_std, alpha=0.1)
    plt.fill_between(test_mean - test_std, test_mean + test_std, alpha=0.1)

    plt.xlabel('Training Examples')
    plt.ylabel('R2 Score')
    plt.title(f'Learning Curves ({model_name})')
    plt.legend(loc='best')
    plt.grid(True)
    plt.show()

# Seçili modeller için learning curves
for name in ['Random Forest', 'SVR', 'Gradient Boosting']:
    model = models[name].best_estimator_ if isinstance(models[name], RandomizedSearchCV) else models[name]
    plot_learning_curves(model, X_train, y_train, name)

# Model karşılaştırma
print("\nModel Karşılaştırma")
print("-" * 50)

# Sonuçları DataFrame'e çevirme
results_df = pd.DataFrame(results).T
print("\nTüm Model Metrikleri:")
print(results_df)

# Metrikleri görselleştirme
metrics_to_plot = ['R2 Score', 'MAE', 'RMSE', 'Cross Val Score']
fig, axes = plt.subplots(2, 2, figsize=(15, 10))
fig.suptitle('Model Karşılaştırma Metrikleri')

for i, metric in enumerate(metrics_to_plot):
    ax = axes[i//2, i%2]
    results_df[metric].plot(kind='bar', ax=ax)
    ax.set_title(metric)
    ax.set_xticklabels(ax.get_xticklabels(), rotation=45)

plt.tight_layout()
plt.show()

# Model tahmin süreleri analizi
print("\nModel Tahmin Süreleri Analizi")
print("-" * 50)

prediction_times = {}

for name, model in models.items():
    start_time = time.time()
    model.predict(X_test)
    end_time = time.time()
    prediction_times[name] = end_time - start_time

# Tahmin sürelerini görselleştirme
plt.figure(figsize=(10, 6))
plt.bar(prediction_times.keys(), prediction_times.values())
plt.xticks(rotation=45)
plt.title('Model Tahmin Süreleri (saniye)')
plt.tight_layout()
plt.show()

# Sonuçları kaydetme
print("\nSonuçları Kaydetme")
print("-" * 50)

# Model performans sonuçlarını kaydetme
results_df.to_csv('model_performance_results.csv')
print("Model performans sonuçları kaydedildi.")

# Özellik önem derecelerini kaydetme
feature_importance.to_csv('feature_importance_rf.csv')
svr_importance.to_csv('feature_importance_svr.csv')
gb_importance.to_csv('feature_importance_gb.csv')
print("Özellik önem dereceleri kaydedildi.")

# En iyi modeli kaydetme
import joblib
best_model_name = results_df['R2 Score'].idxmax()
best_model_object = models[best_model_name]
joblib.dump(best_model_object, 'best_model.joblib')
print("En iyi model kaydedildi.")

# Özet Rapor
print("\nÖzet Rapor")
print("-" * 50)
print("Analiz Sonuçları:")
print(f"1. En iyi model: {best_model_name}")
print(f"2. En iyi R2 skoru: {results_df.loc[best_model_name, 'R2 Score']:.4f}")

print("\nEn etkili faktörler:")
for idx, row in feature_importance.head(5).iterrows():
    print(f"- {row['feature']}: {row['importance']:.4f}")

# Model Karşılaştırması
print("\nModel Karşılaştırması:")
for model_name in results_df.index:
    print(f"\n{model_name}:")
    print(f"R2 Score: {results_df.loc[model_name, 'R2 Score']:.4f}")
    print(f"RMSE: {results_df.loc[model_name, 'RMSE']:.4f}")
    print(f"MAE: {results_df.loc[model_name, 'MAE']:.4f}")
    print(f"Explained Variance: {results_df.loc[model_name, 'Explained Variance']:.4f}")
    print(f"MAPE: {results_df.loc[model_name, 'MAPE']:.4f}")
    print(f"Cross Validation Score: {results_df.loc[model_name, 'Cross Val Score']:.4f}")